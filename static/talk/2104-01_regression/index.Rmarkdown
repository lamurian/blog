---
title: Intuition on Regression
author: Aly Lamuri
output:
  xaringan::moon_reader:
    css: ["logo.css", "shinobi", "ninjutsu"]
    seal: false
    self_contained: false
    nature:
      ratio: "16:9"
      countIncrementalSlides: false
      highlightSlides: false
---

```{r init, echo=FALSE}
pkgs <- c("magrittr", "ggplot2", "kableExtra", "ggfortify")
pkgs.load <- sapply(pkgs, library, character.only=TRUE)
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, messages=FALSE, warning=FALSE, error=FALSE,
	fig.width=10, fig.height=5, out.width="90%", dev="jpeg", dpi=300
)
```

count: false
class: bg-main1 split-70 hide-slide-number

.column[.vmiddle.right.content[
.font3[Intuition on .amber[Regression]]
]]

.bg-main4.column[.vmiddle.content[
.amber[Aly Lamuri]  
Indonesia Medical Education and Research Institute
]]

.right-logo[]

---

name: overview
layout: true
class: bg-main4 split-30 hide-slide-number

.column[.vmiddle.right.content[
.font3.amber[Overview]
]]

---

template: overview
count: false

.bg-main1.column[.vmiddle.content[
- .amber[Basics on regression: LM]
- Extending linearity: GLM
- A bit on Factor Analysis
- Path Analysis
- Structural Equation Model
]]

.right-logo[]

---

layout: false
count: false
class: bg-main3 hide-slide-number

# Prior concepts to understand

.font2[
- Independent variable
- Dependent variable
- Variance-covariance matrix
]

.right-logo[]

???

- IV = feature in ML
- DV = target / label in ML
- Variance-covariance matrix is a square matrix
- The diagonal of var-covar matrix is the Variance
- While the upper and lower triangles are the covariances

---

layout: false
class: bg-main3

# Basic on regression: .amber[LM]

.font2[
- LM stands for Linear Model
- Relies on variance-covariance matrix
- It extends .amber[correlation]
- Describing linearity between IVs and DV
]

.right-logo[]

---

count: false
class: bg-main3

# What is .amber[linearity] anyway?

.font2[
- Relationship between two variables
- A specific form of .amber[monotonic] relationships
- Variables could be observable or inferrable (more on this on later sections)
- .amber[Hint:] upwards and downwards .amber[slope]
]

.right-logo[]

---

class: bg-main3

# Monotonic relationship

```{r monotonic}

seed <- 1
size <- 200

set.seed(seed)

b0  <- 1.5
b1  <- 2.1
x   <- runif(size, 0, 1.5)
mu  <- exp(b0 + b1*x)
y   <- rpois(n=size, lambda=mu)
tbl <- data.frame(list("x"=x, "y"=y))

ggplot(tbl, aes(x, y)) +
	theme_minimal() +
	geom_point(alpha=1) +
	labs(x="", y="")

```

.right-logo[]

--

.font2[We will revisit this trend later :)]

---

count: false
class: bg-main3

# Linearity

```{r linear}

set.seed(seed)

x   <- rnorm(size, 3, 1)
y   <- x + runif(size, -1, 1)
tbl <- data.frame(list("x"=x, "y"=y))

ggplot(tbl, aes(x, y)) +
	theme_minimal() +
	geom_point(alpha=1) +
	labs(x="", y="")

```

.right-logo[]

---

layout: true
class: bg-main3

# What does a model do?

.right-logo[]

---

.center.font2[
It transforms this...

\begin{bmatrix}
\sigma_{1, 1} & \cdots & \sigma_{1, n} \\
\vdots	      & \ddots & \vdots       \\
\sigma_{n, 1} & \cdots & \sigma_{n ,n}
\end{bmatrix}

]

.center.font2[.amber[Hint:] This is a variance-covariance matrix]

???

- An input matrix of independent variables
- This data contains $n$ variables from $m$ subjects
- Each row maps to dependent variable $y_i$

---

count: false
class: bg-main3

.center.font2[
...into this...

$$\displaystyle \beta_0 + \sum_{i=1}^n \beta_i X_i$$
]

--

.center.font2[
...so that we can find $\color{orange}{\hat{y}}$
]

--

.center.font2[Hold on, hold on...]

---

count: false

.center[
<br> <br>

![](https://thinkingmeme.com/wp-content/uploads/2018/02/memesfaces9.jpg)

.font2[
Why $\color{orange}{\hat{y}}$ and not $\color{orange}{y}$ though?
]
]

---

count: false

.font2[
- A model provides a prediction
- Thus the notation: $\color{orange}{\hat{y}}$
- It explains the variability in .amber[DV] using variance-covariance matrix of
  .amber[IVs]
- Perfect linearity $\to$ The trend of IV completely defines DV
]

---

layout: true
class: bg-main3

.right-logo[]

# Linear Model

---

.font2[
- Now, we understand what models do
- We have been able to distinguish linear to monotonic trend
- We will revisit linearity and fit in a model to our data :)
]

---

## What we previously had

```{r linear}
```

---

count: false

## How about fitting our data to a model?

```{r linear.fit}

set.seed(seed)

x   <- rnorm(size, 3, 1)
y   <- x + runif(size, -1, 1)
tbl <- data.frame(list("x"=x, "y"=y))
mod <- lm(y ~ x, data=tbl)

tbl$predict <- predict(mod)

ggplot(tbl, aes(x, y)) +
	theme_minimal() +
	geom_point(alpha=1) +
	geom_line(aes(y=predict), color="indianred", size=1.2) +
	ggpubr::stat_regline_equation(formula=y~x, label.x=5.3, label.y=4.1, color="indianred", size=5, hjust=1, geom="label") +
	labs(x="", y="")

```

---

count: false

## Summarizing the model

```{r linear.summary}
summary(mod)
```

---

count: false

## Visual evaluation on the model

```{r linear.eval1}
autoplot(mod) + theme_minimal()
```

---

count: false

## Statistical evaluation on the model

```{r linear.eval2, echo=TRUE}
# Residual normality
mod %>% residuals() %>% nortest::ad.test()

# Homogeneity of residual variance
mod %>% lmtest::hmctest()
```

---

count: false

.center[
.font2[How do we conclude our model?]

<img src="https://www.wallpaperup.com/uploads/wallpapers/2014/02/11/252308/c95c61be7a901ea649fa12ec08267ab6.jpg", width="50%">

.amber.font2[Acceptable]
]

???

- For most values, our model fit perfectly
- For extreme values, the residual is seemingly not normally distributed
- We need to take into account potential outliers

---

layout: false
class: bg-main3

.right-logo[]

# However...

.font2[
- .amber[Linear Model] only accepts linear trends
- It could not correctly model monotonic relationship
- Let alone .pink[non-monotonic] relationship :( 
]

--

# What can we do?

---

template: overview
count: false

.bg-main1.column[.vmiddle.content[
- Basics on regression: LM
- .amber[Extending linearity: GLM]
- A bit on Factor Analysis
- Path Analysis
- Structural Equation Model
]]

.right-logo[]

---

layout: true
class: bg-main3

.right-logo[]

# Generalized Linear Model

---

.font2[
- Introducing link function to the model
- Link function will change the way our model predict the outcome
- Link function .amber[*does not*] transform the data
- It just defines how the model behave!
]

---

## What we previously had

```{r monotonic}
```

---

count: false

## How about fitting our data to a .amber[linear] model?

```{r monotonic.fit1}

set.seed(seed)

x   <- rnorm(size, 3, 1)
y   <- abs(x^3) + runif(size, -8, 8)
tbl <- data.frame(list("x"=x, "y"=y))
mod <- lm(y ~ x, data=tbl)

tbl$predict <- predict(mod)

ggplot(tbl, aes(x, y)) +
	theme_minimal() +
	geom_point(alpha=1) +
	geom_line(aes(y=predict), color="indianred", size=1.2) +
	ggpubr::stat_regline_equation(formula=y~x, label.x=3, label.y=45, color="indianred", size=5, hjust=1, geom="label") +
	labs(x="", y="")

```

---

count: false

.center[
.font2[Our model does not provide a proper fit!]

<img src="https://theawesomedaily.com/wp-content/uploads/2017/07/meme-faces-6-1.jpg" width="40%">

.font2.amber[Okay, what's next?]
]

---

## Fit the data into a .amber[polynomial] model

```{r monotonic.fit2}

set.seed(seed)

b0  <- 1.5
b1  <- 2.1
x   <- runif(size, 0, 1.5)
mu  <- exp(b0 + b1*x)
y   <- rpois(n=size, lambda=mu)
tbl <- data.frame(list("x"=x, "y"=y))
mod <- lm(y ~ poly(x, 2), data=tbl)

tbl$predict <- predict(mod)

ggplot(tbl, aes(x, y)) +
	theme_minimal() +
	geom_point(alpha=1) +
	geom_line(aes(y=predict), color="indianred", size=1.2) +
	ggpubr::stat_regline_equation(formula=y~poly(x, 2), label.x=1, label.y=50, color="indianred", size=5, hjust=1, geom="label") +
	labs(x="", y="")

```

---

count: false

## Visual evaluation

```{r monotonic.eval}
autoplot(mod) + theme_minimal()
```

---

## ...or use GLM: .amber[Poisson] regression

```{r glm.pois}

set.seed(seed)

b0  <- 1.5
b1  <- 2.1
x   <- runif(size, 0, 1.5)
mu  <- exp(b0 + b1*x)
y   <- rpois(n=size, lambda=mu)
tbl <- data.frame(list("x"=x, "y"=y))

mod <- glm(y ~ x, data=tbl, family=poisson)

eq  <- coef(mod) %>% {sprintf("log(y) = %.2f + %.2fx", .[[1]], .[[2]])}

tbl$predict <- exp(predict(mod))

ggplot(tbl, aes(x, y)) +
	theme_minimal() +
	geom_point(alpha=1) +
	geom_line(aes(y=predict), color="indianred", size=1.2) +
	annotate("label", label=eq, x=1, y=50, size=5, hjust=1, color="indianred") +
	labs(x="", y="")

```

---

count: false

## Visual evaluation

```{r glm.pois.eval}
autoplot(mod) + theme_minimal()
```

---

## Another cool stuff with GLM: .amber[Logistic] regression

```{r glm.binom1}

set.seed(seed)

b0  <- 1.5
b1  <- 2.1
x   <- rnorm(size)
sim <- b0 + b1*x
p   <- exp(sim) / (1 + exp(sim))
y   <- rbinom(size, 1, p)
tbl <- data.frame(list("x"=x, "y"=y))

mod <- glm(y ~ x, data=tbl, family=binomial)

eq  <- coef(mod) %>% {sprintf("logit(y) = %.2f + %.2fx", .[[1]], .[[2]])}

tbl$predict <- predict(mod) %>% {exp(.) / (1+exp(.))}

ggplot(tbl, aes(x, y)) +
	theme_minimal() +
	geom_point(alpha=1) +
	geom_line(aes(y=predict), color="indianred", size=1.2) +
	annotate("label", label=eq, x=0, y=0.5, size=5, color="indianred") +
	labs(x="", y="")

```

---

count: false

## Also with GLM: .amber[Probit] regression

```{r glm.binom2}

set.seed(seed)

b0  <- 1.5
b1  <- 2.1
x   <- rnorm(size)
sim <- b0 + b1*x
p   <- pnorm(sim)
y   <- rbinom(size, 1, p)
tbl <- data.frame(list("x"=x, "y"=y))

mod <- glm(y ~ x, data=tbl, family=binomial(link="probit"))

eq  <- coef(mod) %>% {sprintf("probit(y) = %.2f + %.2fx", .[[1]], .[[2]])}

tbl$predict <- predict(mod) %>% {exp(.) / (1+exp(.))}

ggplot(tbl, aes(x, y)) +
	theme_minimal() +
	geom_point(alpha=1) +
	geom_line(aes(y=predict), color="indianred", size=1.2) +
	annotate("label", label=eq, x=0, y=0.5, size=5, color="indianred") +
	labs(x="", y="")

```

---

layout: false
class: bg-main3

.right-logo

# And .amber[many more] to count...

.font2[
.amber[Use cases:]

- LM $\to$ Linear trend
- Poisson $\to$ DGP with rate of occurrence
- Logistic $\to$ DGP under true bernoulli trial (binomial outcome)
- Probit $\to$ DGP for binomial outcome with underlying Gaussian process
- Gamma $\to$ DGP in exponential growth / decay events
- Quasi-poisson / binomial $\to$ Poisson / Binomial DGP with overdispersion
- Polynomial regression $\to$ Non-linear data, unknown DGP
]

???

Examples:

- Poisson: Factors influencing weekly defecation rate
- Logistic: Factors influencing a specific diagnosis (outcome in binomial)
- Probit: Factors influencing drug response (outcome in binomial)
- Gamma: Factors influencing required time in cell division

---

template: overview
count: false

.bg-main1.column[.vmiddle.content[
- Basics on regression: LM
- Extending linearity: GLM
- .amber[A bit on Factor Analysis]
- Path Analysis
- Structural Equation Model
]]

.right-logo[]

---

class: bg-main3

.right-logo[]

# Factor Analysis

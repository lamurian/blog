---
author: Aly Lamuri
title: >
  Deep Learning and Connectome
output:
  revealjs::revealjs_presentation:
    self_contained: false
    theme: solarized
---

```{r init, echo=F, include=F, message=F, eval=T}
options(digits=2)
knitr::opts_chunk$set(
	fig.width=7, fig.height=4, results='asis', echo=F,
	message=F, fig.align="center", python.reticulate=F
)
library(ggplot2)
load("result.RData")
```

# Introduction

Depression:

> - Numerically? Stupendous
- Anhedonia & anergy
- An affective disorder **yet** a constellation of *numerous* symptoms
- Severe economic burden
- Irony: treatment failure in roughly half of patients
- Finding a biomarker is difficult!
- But, how if we look at depression as a result of brain regional interaction?

## Brain Connectivity and Depression

- Higher ALFF in hippocampus and amygdala
- Uncoupling of cerebellum and caudate nucleus
- Connectivity changes after therapeutic session
- *Emerging concept:* Connectivity changes as outlined by the graph theory
- **Current focus:** Implementing deep learning to comprehend brain network

## Proposed Model

- Uses Graph Convolutional Network
- Group: Healthy, mild, moderate, severe
- Construct a sparse block diagonal matrix
- Embed vertex influence metrics as nodal feature
- **Output:** A multinomial node-classification model

# Methods

- Cross-sectional observational-analytic study
- Data from the UK Biobank
- Depression category based on the UKB classification
- Build adjacency matrix from upper triangle rs-fMRI
- Apply proportional threshold

## Graph Convolutional Network

Inputs: Adjacency matrix, nodal feature, nodal label

$$H^{(l+1)} = \sigma (\hat{D}^{- \frac{1}{2}} \cdot \hat{A} \cdot \hat{D}^{- \frac{1}{2}} \cdot H^{(l)} \cdot W^{(l)} + b)$$

## Data Preparation

- Separate subjects into groups
- Reconstruct adjacency matrix
- Proportional thresholding (80%) and compute metrics
- Concatenate matrices

## Statistical Analysis

- Normality: Anderson-Darling test
- ANOVA or Kruskal-Wallis
- Alpha: 0.05
- Post-Hoc: Tukey's HSD or Dunn's Test with Benjamini-Horschberg
- Metrics differences across ROIs

## Experimental Design

- Visualize higher-dimension: T-SNE (hierarchical clustering)
- Model using `stellargraph` in `python3.0`
- Back end: `tensorflow` through `keras` implementation
- Use `networkx` to parse graph objects

# Model

```{python mod, eval=F, echo=T, python.reticulate=F}
from tensorflow.keras import Model
from stellargraph import Ensemble
from stellargraph.layer import GCN, GraphConvolution
```

## Neural network layer

```{python mod.layer, eval=F, echo=T, python.reticulate=F}
gcn_layer = GCN(
        [64, 64, train_target.shape[1]],
        generator=generator,
        activations=['relu', 'relu', 'softmax'],
        dropout=0.5)
```


## Create input and output layers

```{python mod.io, eval=F, echo=T, python.reticulate=F}
g_input, g_output = gcn_layer.node_model()
model = Model(inputs=g_input, outputs=g_output)
```

## Use ensemble and compile the model

```{python mod.ensemble, eval=F, echo=T, python.reticulate=F}
model = Ensemble(model, n_estimators=5)
model.compile(optimizer='adam', loss='categorical_crossentropy',
        metrics=['accuracy'])
```

# Results

```{r plot.tsne}
plot.tsne
```

## Deep Learning Accuracy

```{r plot.acc, messages=F}
plot.acc
```

## Classification ROC

```{r plot.roc}
plot.roc
```

# Discussion

- High variance in brain network
- Dimensionality reduction technique could not sufficiently cluster data
- Geometric deep learning: algorithm on graph and manifold?

---
title: "Popular Machine Learning Algorithm"
author: Aly Lamuri
output:
  xaringan::moon_reader:
    seal: false
    css: ["shinobi", "ninjutsu"]
    self_contained: false
    nature:
      ratio: "16:9"
      countIncrementalSlides: false
      highlightLines: true
---

```{r init, echo=FALSE}
pkgs <- c("magrittr")
pkgs.load <- sapply(pkgs, library, character.only=TRUE)
knitr::opts_chunk$set(echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, error=FALSE,
	dpi=300, dev="png", fig.width=10, fig.height=5, output.widht="100%"
)
```

count: false
class: bg-main1 split-70 hide-slide-number

.column[.right.vmiddle.content[
.font3[Popular Machine Learning .amber[Algorithm]]
]]

.bg-main4.column[.vmiddle.content[
.amber[Aly Lamuri]  
Department of Medical Physics
]]

---

name: overview
layout: true
count: false
class: bg-main4 split-30 hide-slide-number middle

.column[.right.vmiddle.content[
.font3.amber[Overview]
]]

---

template: overview
count: false

.bg-main1.column[.vmiddle.content[
- .amber[Preparatory steps]
- Regression
- Classification
- Clustering
- Dimensionality reduction
]]

---

layout: false
class: bg-main3 middle center

# Type of machine learning

<img src="https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190522174744/MachineLearning.png" width="70%">

???

- This course focuses on supervised and unsupervised machine learning
- Supervised: the machine learns from your label
- Unsupervised: the machine creates label based on patterns
- Semi-supervised: you have label, but some are missing
- Reinforcement ML: a self-learning model

---

layout: true
class: bg-main3 split-two

.column[.vmiddle.center.content[
{{content}}
]]

.column[.font2.vmiddle.content[
# Preparatory steps

- Problem definition
- Data collection
- Data preparation
- Training
- Evaluation
]]

---

<img src="https://image.freepik.com/free-vector/flat-thinking-concept_23-2148163823.jpg" width="90%">

Formulating a research question

???

- Define the problem you want to solve
- What are the variables?
- State your hypothesis!

---

count: false

<img src="https://www.tibco.com/blog/wp-content/uploads/2016/03/rsz_bigstock-business-analytics-line-style-97638011.jpg" width="90%">

Collecting necessary data

???

- From your research
- Use datasets from a repository:
  - ADNI: Alzheimer's Disease Neuroimaging Initiative
  - GEO: Gene Expression Omnibus
  - Brain / MINDS
  - WormBase: online database of *C. elegans*
- Use previously unused data: EHR, biobank, etc

---

count: false

<img src="https://media.istockphoto.com/vectors/concepts-for-creative-process-big-data-filter-data-tunnel-analysis-vector-id464806966?k=6&m=464806966&s=612x612&w=0&h=yVt-f-mmhMpTNtIq_Vitf03Ah2-Fyv-xTXvijRwEupw%3D" width="90%">

Exploring your data

???

- Deal with NA (empty cells)
- Exploratory data analysis: univariate and bivariate
  - Summary statistics
  - Central tendency
  - Spread
  - Data distribution
  - Correlation
  - Proportional difference
  - Mean difference

---

count: false

<img src="https://image.freepik.com/free-vector/corporate-training-company-business-employees_82574-5244.jpg" width="90%">

Choose an appropriate model

???

- Our main focus for today
- Cross validation: do you need it?
- Epoch: how much training?
- Parameters to tune

---

count: false

<img src="https://image.freepik.com/free-vector/business-background-design_1200-17.jpg" width="90%">

???

- Does your model perform well?
- What to evaluate?
- On the next lecture

---

layout: false
class: bg-main3

# .amber[Cross-validation:] exhaustive vs. non-exhaustive?

.font2[
- Hold-out method
- $k$-fold cross-validation
- Leave- $p$ -out cross-validation
- Time-series data?
]

???

- Type: exhaustive and non-exhaustive
- Exhaustive: try all possible combination for separating training and validation set
- Hold-out: separate data into training and testing
- $k$-fold CV: separate data into $k$ number of bins, then perform validation on each epoch
- Leave-$p$-out CV: for each data point, leave a number of $p$ for validation
- Does not work well in time-series data $\to$ solution: use rolling CV

---

template: overview
count: false

.bg-main1.column[.vmiddle.content[
- Preparatory steps
- .amber[Regression]
- Classification
- Clustering
- Dimensionality reduction
]]

---

class: bg-main3

# .amber[Regression]

.font2[
- K-Nearest Neighbor
- Random Forest
- Regularized Linear
]

???

Application: predicting continuous data

---

class: bg-main3

# KNN .amber[Regression]

.font2[
- Store all available cases
- Do similarity measures, e.g. distance function
- Take the average value of closely-located data (hence the name!)
- Easy to implement, low calculation time
]

???

Concept recall on distance function:
- Euclidean distance: shortest distance, efficient for low-di
- Manhattan distance: absolute distance, good for hi-di
- Minkowski distance: bit of both, in non-euclidean space

Hamming distance $\to$ for categorical variable:
- Assign 1 to those with same values
- Otherwise: 0

--

## Hyperparameters

.font2[
- Number of neighbors $k$
- Distance parameter $p$ from the Minkowski function
]

???

Hint: recall the Minkowski distance function
- $p = 1 \to$ Manhattan
- $p = 2 \to$ Euclidean
- Other $p \to$ Minkowski

---

count: false
class: bg-main3

<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/08/Screenshot-from-2018-08-22-12-50-041.png" width="90%">

---

class: bg-main3

# RF .amber[Regression]

.font2[
- Core concept: random forest is an .amber[ensemble] of decision trees
- Compute the average value from each tree
- Reducing variances produced by a single tree
]

???

Why is it a blackbox?
- The tree decides on an average values given specific variable features
- Larger features $\to$ better segregation
- Does not extrapolate that well $\to$ blind to a new dataset

--

## Hyperparameters

.font2[
- Depth of the tree
- Number of: trees .pink[vs] feature
- Minimum number of data: in a node .pink[vs] leave
]

???

- Depth: How far the splitting goes
- Number of trees: trees in a random forest
- Number of features: total features to consider splitting
- Data in a node: minimum number of data in a node before splitting
- Data in a leaf: minimum number of data allowed in a leaf

---

count: false
class: bg-main3

<img src="https://miro.medium.com/max/2612/0*f_qQPFpdofWGLQqc.png" width="90%">

---

class: bg-main3

# Regularized Linear .amber[Regression]

.font2[
- Using regularized least square
- Indication to use: features > instance
]

???

- LM: use ordinary least square
- Features: observed variables
- Instance: number of cases / observations
- In OLS: we try to keep a minimal number of features (e.g. using a stepwise
  procedure)

--

## Type of regularization

.font2[
- Lasso
- Ridge regression
- Elastic net
]

???

Regularization:
- Lasso: cost function + {sum of coef. (L-1 regularization) * a constant}
- Ridge: cost function + {sum of squared coef. (L-2 regularization) * a constant}
- Elastic net: Use both L-1 and L-2 regularization

---

template: overview
count: false

.bg-main1.column[.vmiddle.content[
- Preparatory steps
- Regression
- .amber[Classification]
- Clustering
- Dimensionality reduction
]]

---

class: bg-main3

# .amber[Classification]

.font2[
- K-Nearest Neighbor
- Random Forest
- Discriminant Analysis
]

???

Application: multinomial classification, see it as an extension to logistic regression

---

class: bg-main3

# KNN .amber[Classification]

.font2[
- Remember how LM generalizes into GLM?
- Just like so, KNN classifier is similar to KNN regressor
- Instead of averaging, it .amber[votes] $k$-closest neighbors
]

---

count: false
class: bg-main3 middle center

<img src="http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final1_ibdm8a.png" width="70%">

---

class: bg-main3

# RF .amber[Classification]

.font2[
- A generalization of a decision tree
- What does a decision tree do? Making a decision (duh).
- Works similar to its regressor counterparts
- .amber[Voting] instead of averaging the value
]

---

count: false
class: bg-main3

<img src="https://dulfd6logpzir.cloudfront.net/wp-content/uploads/2016/06/decision-tree-example.jpeg" width="90%">

---

class: bg-main3

# Discriminant Analysis

.font2[
- Discriminate features and classify them accordingly
- Has stringent statistical assumptions
- Violation $\to$ does not generalize well
- Type: LDA and QDA
]

--

## Statistical assumption

.font2[
- Multivariate normality
- LDA: Multivariate homogeneity of variance
]

???

- QDA does not require multivariate homogeneity of variance

---

count: false
class: bg-main3 middle center

<img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_lda_qda_0011.png" width="70%">

---

template: overview
count: false

.bg-main1.column[.vmiddle.content[
- Preparatory steps
- Regression
- Classification
- .amber[Clustering]
- Dimensionality reduction
]]

---

class: bg-main3

# .amber[Clustering]

.font2[
- Density-based
- Hierarchical
- K-Means
- Random Forest
]

---

class: bg-main3

# Density-based .amber[Clustering]

.font2[
- Looks for 'clumps' in a data
- Defines cluster as closely-located instances
- Estimates densities
]

???

- Estimating density by calculating the core distance, i.e. the distance of a
  point to its $k$-th nearest neighbor
- Denser region $\to$ smaller core distance
- Density = 1 / core distance
- Build a minimum spanning tree by pruning long-distanced data

--

## Hyperparameters

.font2[
- Minimum sample to compute the core distance
- Minimum size to consider a cluster
]

---

count: false
class: bg-main3 center middle

<img src="https://www.researchgate.net/profile/Peter_Laurinec/post/How_would_you_choose_the_most_representative_points_from_clusters_created_by_density-based_clustering_method/attachment/59d626f46cda7b8083a23e37/AS%3A521763134951424%401501409433380/image/representatives.png" width="75%">

---

class: bg-main3

# Hierarchical .amber[Clustering]

.font2[
- Based on a greedy algorithm
- Choose a cluster based on closest distance
- Usually rely on the Euclidean distance
]

???

Other distance measures are still applicable though

---

count: false
class: bg-main3 center middle

<img src="https://dashee87.github.io/images/hierarch.gif" width="100%">

---

class: bg-main3

# K-Means .amber[Clustering]

.font2[
- Objective: minimizing intra-cluster variation
- Several algorithms available, Hartigan-Wong is the standard
- Data imputed as a z-score
]

???

Hartigan-Wong:
- Intra-cluster variation: sum of squared Euclidean distance between item and a centroid
- $k$ centroid initiated randomly
- Calculate the Euclidean distance from a data point to each cluster
- Choose the closest cluster, assign membership
- Calculate the mean of a cluster
- Choose the new centroid based on calculated mean
- Rinse and repeat until convergence or reaching max iteration

---

count: false
class: bg-main3 middle center

<img src="https://i.stack.imgur.com/kVx8d.gif" width="60%">

---

count: false
class: bg-main3 middle center

<img src="https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif" width="60%">

???

Problem:
- Clusters depend on the initial assignment
- Different assignment $\to$ different outcomes!

---

class: bg-main3

# Random Forest .amber[Clustering]

.font2[
- Does not require transformation
- Save effort in feature preprocessing (yay)
- Quite a powerful machine learning model
]

???

- Recall that K-Means clustering requires the data as a z-score
- First it creates a synthetic data of the same size as the original one
- Assigns different label to each data
- Make a random forest to classify both labels
- Compute similarity score based on extracted data instances
- Use other clustering algorithms to create the cluster (usually a hierarchical clustering)

---

template: overview
count: false

.bg-main1.column[.vmiddle.content[
- Preparatory steps
- Regression
- Classification
- Clustering
- .amber[Dimensionality reduction]
]]

---

class: bg-main3

# Dimensionality Reduction

.font2[
- Principal Component Analysis
- Independent Component Analysis
- Factor Analysis
]

???

- Will not discuss the mathematics
- All involve orthogonal / oblique axial rotation
- Quite complex and daunting mathematically
- But intuitive to understand conceptually

---

class: bg-main3

# Principal Component Analysis

.font2[
- Maximizes variances in each principal component
- The first principal component always has the highest variance
- Useful tool in exploratory data analysis
- Visualizes high dimensional data
- Cleaning before applying ICA
]

---

count: false
class: bg-main3 center middle

<img src="https://poissonisfish.files.wordpress.com/2017/01/q7hip.gif?w=1108" width="90%">

---

count: false
class: bg-main3 center middle

<img src="https://cdn-images-1.medium.com/max/1600/1*XGaA7KWUlhWZLIezYEBIHA.gif" width="90%">

---

count: false
class: bg-main3 center middle

<img src="http://matthewdeakos.me/wp-content/uploads/2018/02/ezgif.com-crop-4.gif" width="70%">

---

class: bg-main3

# Independent Component Analysis

.font2[
- Separate multivariate (joint) distribution into its additive components
- Often requires PCA to filter out noises
- Example: the cocktail party effect
]

???

- How does our brain keep up with all gibberish noises during a cocktail party?
- Can we replicate our auditory function mathematically?

---

count: false
class: bg-main3 center middle

<img src="http://image3.slideserve.com/6601960/independent-component-analysis-ica-the-cocktail-party-problem-n.jpg" width="70%">

???

- Imagine we have a few recorder placed in several corners during a cocktail party
- ICA can "clean" the noisy data and extract important information

---

class: bg-main3

# Factor Analysis

.font2[
- A part of structural equation model in statistics
- Aim to reduce data dimension into latent variables
- Exploratory vs. confirmatory factor analysis?
]

---

class: bg-main3

# Is that all?

- .font2[No.]

--
- .font2[We just barely scratching the surface :)]

--
- .font2[There are still other algorithms: SVM, ensemble, perceptron, etc...]

--
- .font2[But please mind the distinction between supervised and unsupervised learning]

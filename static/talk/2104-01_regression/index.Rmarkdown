---
title: Intuition on Regression
author: Aly Lamuri
output:
  xaringan::moon_reader:
    css: ["logo.css", "shinobi", "ninjutsu"]
    seal: false
    self_contained: false
    nature:
      ratio: "16:9"
      countIncrementalSlides: false
      highlightSlides: false
---

```{r init, echo=FALSE}
pkgs <- c("magrittr", "ggplot2", "kableExtra", "ggfortify")
pkgs.load <- sapply(pkgs, library, character.only=TRUE)
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, messages=FALSE, warning=FALSE, error=FALSE,
	fig.width=10, fig.height=5, out.width="90%", dev="jpeg", dpi=300
)
```

count: false
class: bg-main1 split-70 hide-slide-number

.column[.vmiddle.right.content[
.font3[Intuition on .amber[Regression]]
]]

.bg-main4.column[.vmiddle.content[
.amber[Aly Lamuri]  
Indonesia Medical Education and Research Institute
]]

.right-logo[]

---

name: overview
layout: true
class: bg-main4 split-30 hide-slide-number

.column[.vmiddle.right.content[
.font3.amber[Overview]
]]

---

template: overview
count: false

.bg-main1.column[.vmiddle.content[
- .amber[Basics on regression: LM]
- Extending linearity: GLM
- A bit on Factor Analysis
- Path Analysis
- Structural Equation Model
]]

.right-logo[]

---

layout: false
count: false
class: bg-main3 hide-slide-number

# Prior concepts to understand

.font2[
- Independent variable
- Dependent variable
- Variance-covariance matrix
]

.right-logo[]

???

- IV = feature in ML
- DV = target / label in ML
- Variance-covariance matrix is a square matrix
- The diagonal of var-covar matrix is the Variance
- While the upper and lower triangles are the covariances

---

layout: false
class: bg-main3

# Basic on regression: .amber[LM]

.font2[
- LM stands for Linear Model
- Relies on variance-covariance matrix
- It extends .amber[correlation]
- Describing linearity between IVs and DV
]

.right-logo[]

---

count: false
class: bg-main3

# What is .amber[linearity] anyway?

.font2[
- Relationship between two variables
- A specific form of .amber[monotonic] relationships
- Variables could be observable or inferrable (more on this on later sections)
- .amber[Hint:] upwards and downwards .amber[slope]
]

.right-logo[]

---

class: bg-main3

# Monotonic relationship

```{r monotonic}

seed <- 1
size <- 200

set.seed(seed)

x   <- rnorm(size, 3, 1)
y   <- abs(x^3) + runif(size, -8, 8)
tbl <- data.frame(list("x"=x, "y"=y))

ggplot(tbl, aes(x, y)) +
	theme_minimal() +
	geom_point(alpha=1) +
	labs(x="", y="")

```

.right-logo[]

--

.font2[We will skip this relationship at the moment :)]

---

count: false
class: bg-main3

# Linearity

```{r linear}

seed <- 1

set.seed(seed)

x   <- rnorm(size, 3, 1)
y   <- x + runif(size, -1, 1)
tbl <- data.frame(list("x"=x, "y"=y))

ggplot(tbl, aes(x, y)) +
	theme_minimal() +
	geom_point(alpha=1) +
	labs(x="", y="")

```

.right-logo[]

---

layout: true
class: bg-main3

# What does a model do?

.right-logo[]

---

.center.font2[
It transforms this...

\begin{bmatrix}
\sigma_{1, 1} & \cdots & \sigma_{1, n} \\
\vdots	      & \ddots & \vdots       \\
\sigma_{n, 1} & \cdots & \sigma_{n ,n}
\end{bmatrix}

]

.center.font2[.amber[Hint:] This is a variance-covariance matrix]

???

- An input matrix of independent variables
- This data contains $n$ variables from $m$ subjects
- Each row maps to dependent variable $y_i$

---

count: false
class: bg-main3

.center.font2[
...into this...

$$\displaystyle \beta_0 + \sum_{i=1}^n \beta_i X_i$$
]

--

.center.font2[
...so that we can find $\color{orange}{\hat{y}}$
]

--

.center.font2[Hold on, hold on...]

---

count: false

.center[
<br> <br>

![](https://thinkingmeme.com/wp-content/uploads/2018/02/memesfaces9.jpg)

.font2[
Why $\color{orange}{\hat{y}}$ and not $\color{orange}{y}$ though?
]
]

---

count: false

.font2[
- A model provides a prediction
- Thus the notation: $\color{orange}{\hat{y}}$
- It explains the variability in .amber[DV] using variance-covariance matrix of
  .amber[IVs]
- Perfect linearity $\to$ The trend of IV completely defines DV
]

---

layout: true
class: bg-main3

.right-logo[]

# Linear Model

---

.font2[
- Now, we understand what models do
- We have been able to distinguish linear trend to monotonic relationship
- We will revisit linearity and fit in a model to our data :)
]

---

## What we previously had

```{r linear}
```

---

count: false

## How about fitting our data to a model?

```{r linear.fit}

seed <- 1

set.seed(seed)

x   <- rnorm(size, 3, 1)
y   <- x + runif(size, -1, 1)
tbl <- data.frame(list("x"=x, "y"=y))
mod <- lm(y ~ x, data=tbl)

tbl$predict <- predict(mod)

ggplot(tbl, aes(x, y)) +
	theme_minimal() +
	geom_point(alpha=1) +
	geom_line(aes(y=predict), color="indianred", size=1.2) +
	ggpubr::stat_regline_equation(formula=y~x, label.x=5.3, label.y=4.1, color="indianred", size=5, hjust=1, geom="label") +
	labs(x="", y="")

```

---

count: false

## Summarizing the model

```{r linear.summary}
summary(mod)
```

---

count: false

## Visual evaluation on the model

```{r linear.eval1}
autoplot(mod) + theme_minimal()
```

---

count: false

## Statistical evaluation on the model

```{r linear.eval2, echo=TRUE}
# Residual normality
mod %>% residuals() %>% nortest::ad.test()

# Homogeneity of residual variance
mod %>% lmtest::hmctest()
```

---

count: false

## How do we conclude our model?

--

.center[
<img src="https://www.wallpaperup.com/uploads/wallpapers/2014/02/11/252308/c95c61be7a901ea649fa12ec08267ab6.jpg", width="50%">

.amber.font2[Acceptable]
]

???

- For most values, our model fit perfectly
- For extreme values, the residual is seemingly not normally distributed
- We need to take into account potential outliers

---

layout: false
class: bg-main3

.right-logo[]

# However...

.font2[
- .amber[Linear Model] only accepts linear trends
- It could not correctly model monotonic relationship
- Let alone .pink[non-linear] relationship :( 
]

--

# What can we do?

---

template: overview
count: false

.bg-main1.column[.vmiddle.content[
- Basics on regression: LM
- .amber[Extending linearity: GLM]
- A bit on Factor Analysis
- Path Analysis
- Structural Equation Model
]]

.right-logo[]

---

layout: true
class: bg-main3

.right-logo[]

# Generalized Linear Model

---

.font2[
- Introducing link function to the model
- Link function will change the way our model predict the outcome
- Link function .amber[*does not*] transform the data
- It just defines how the model behave!
]

---

## What we previously had

```{r monotonic}
```

---

count: false

## How about fitting our data to a .amber[linear] model?

```{r monotonic.fit1}

seed <- 1

set.seed(seed)

x   <- rnorm(size, 3, 1)
y   <- abs(x^3) + runif(size, -8, 8)
tbl <- data.frame(list("x"=x, "y"=y))
mod <- lm(y ~ x, data=tbl)

tbl$predict <- predict(mod)

ggplot(tbl, aes(x, y)) +
	theme_minimal() +
	geom_point(alpha=1) +
	geom_line(aes(y=predict), color="indianred", size=1.2) +
	ggpubr::stat_regline_equation(formula=y~x, label.x=3, label.y=45, color="indianred", size=5, hjust=1, geom="label") +
	labs(x="", y="")

```

---

count: false

.center[
.font2[Our model does not provide a proper fit!]

<img src="https://theawesomedaily.com/wp-content/uploads/2017/07/meme-faces-6-1.jpg" width="40%">

.font2.amber[What can we do?]
]

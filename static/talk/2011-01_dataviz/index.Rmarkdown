---
title: Statistics to Understand the Data
author: Aly Lamuri
output:
  xaringan::moon_reader:
    css: ["shinobi", "ninjutsu"]
    seal: false
    self_contained: false
    nature:
      ratio: "16:9"
      countIncrementalSlides: false
      highlightSlides: false
---

```{r init, echo=FALSE}
pkgs <- c("magrittr", "ggplot2", "kableExtra")
pkgs.load <- sapply(pkgs, library, character.only=TRUE)
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, messages=FALSE, warning=FALSE, error=FALSE,
	fig.width=10, fig.height=6, out.width="90%", dev="jpeg", dpi=300
)
```

count: false
class: bg-main1 split-70 hide-slide-number

.column[.vmiddle.right.content[
.font3[.amber[Statistics] to Understand the Data]
]]

.bg-main4.column[.vmiddle.content[
.amber[Aly Lamuri]  
Indonesia Medical Education and Research Institute
]]

---

name: overview
layout: true
class: bg-main4 split-30 hide-slide-number

.column[.vmiddle.right.content[
.font3.amber[Overview]
]]

---

template: overview
count: false

.bg-main1.column[.vmiddle.content[
- .amber[Type of data]
- Descriptive statistics
- Data distribution
- Visual examination
- Goodness of fit test
]]

---

layout: true
class: split-two bg-main3

.bg-main5.column[.vmiddle.center.content[
{{content}}
]]

.column[.vmiddle.content[
# Type of data

.font2[
- Categorical
- Numeric
]
]]

---

<img src="https://image.freepik.com/free-vector/choosing-healthy-unhealthy-food_23-2148584074.jpg" width="100%">
Nominal

???

- Example: healthy vs unhealthy food
- In nominal data, we *identify* what distinguishes one data from another
- No applicable mathematical operation
- Does not provide direct comparison

---

count: false

<img src="https://thumbs.dreamstime.com/b/vector-illustration-horizontal-banner-student-growth-chart-climbing-up-to-higher-level-help-books-graduating-99222118.jpg" width="100%">
Ordinal

???

- Example: education status
- There exists a natural order
- Stratified
- No applicable mathematical operation
- Enable direct comparison

--

???

Why does it matter?

- To apply a correct contrast
- Example: when performing a statistical test / model requiring the sum of
  square (e.g. ANOVA or GLM)
- Nominal: Treatment contrast
- Ordinal: Polynomial orthogonal contrast
- A contrast determines the value of your estimates in GLM

---

count: false

<img src="https://image.freepik.com/free-vector/vector-illustration-counting-hand_29937-2444.jpg" width="100%">
Discrete

???

- Countable data
- Limited arithmetical operation: addition and subtraction
- Example: finding a frequency

---

count: false

<img src="https://blog.beamex.com/hs-fs/hubfs/Beamex_blog_pictures/Temperature-graph---2018-06-18-v2.jpg?width=1200&name=Temperature-graph---2018-06-18-v2.jpg" width="100%">
Continuous

???

- Measurable data
- All arithmetical operation
- Interval: No absolute zero, e.g. Fahrenheit, Celsius and Reaumur
- Ratio: Has an absolute zero, e.g. Kelvin and Rankine

---

layout: false
class: bg-main3

.amber[
# Recap
]

.font2[
Categorical:
- Nominal
- Ordinal
]

--

.font2[
Numeric:
- Discrete
- Continuous:
  - Interval
  - Ratio
]

---

template: overview
count: false

.bg-main1.column[.vmiddle.content[
- Type of data
- .amber[Descriptive statistics]
- Data distribution
- Visual examination
- Goodness of fit test
]]

---

class: bg-main3

# What does .pink[statistics] do?

.font2[
- Measures a .amber[sample]
- .amber[Approximates] parameters in the population
- Tells us how the data .amber[behave]
]

???

- Statistics describes a sample
- It estimates the parameter in a given population
- Parameter is the measures of a population
- We often do not know what the parameter is
- Behaviour of a data as reflected by its probability function

---

count: false
class: bg-main3

# What does .pink[statistics] measure?

.font2[
- General description of a data point
- .amber[Central] tendency
- The .amber[spread]
]

---

class: bg-main3

# Central Tendency

.font2[
- Mean
- Median
- Mode
]

???

- Mean = sum of all data divided by $n$, describes ratio and interval
- Median = the midpoint of a data, describes ordinal and all numeric data
- Mode = the most occurring element in a data
- A mean highly depends on symmetricity
- Symmetric data $\to \mu = M$
- Asymmetric data $\to \mu \neq M$

---

class: bg-main3

# Spread

.font2[
- Interquartile Range (IQR)
- Variance
- Standard deviation
]

???

- As its name suggest, the spread indicates (ahem) the spread of your data
- You can divide your data into $k$ parts, in which you regards group of $k$ as quantile
- Quartile is a specific case of quantile, where you separate your data into four groups
- There are others too: decentile, percentile, etc
- Interquartile range measure a range in your quartile
- Variance (and consequently standard deviation) in statistics is a bit
  different from their parameter

---

layout: true
class: bg-main3

# Variance

---

count: false

.font2[
\begin{align}
\sigma^2 &= \frac{\displaystyle \sum_{i=1}^n(x_i - \mu)^2}{n} \\
\end{align}
]

???

- At first, estimating the parameter using statistics seems straightforward
- Let's dig into the equation :)

---

count: false

\begin{align}
&= \frac{\displaystyle \sum_{i=1}^n \bigg(x_i - \frac{1}{n}\sum_{i=1}^n x_i \bigg)^2}{n} \\
&= \frac{\displaystyle \sum_{i=1}^n \bigg(x_i - \frac{x_1 + ... + x_n}{n} \bigg)^2}{n} \\
&= \frac{\displaystyle \sum_{i=1}^n \bigg(\frac{n \cdot x_i - (x_1 + ... + x_n)}{n} \bigg)^2}{n} \\
\end{align}
---

count: false

.font2[
\begin{align}
\frac{\bigg(\frac{\color{#E91E63}{(n-1)} \cdot x_1 - (x_2 + ... + x_n)}{n} + ... + \frac{\color{#E91E63}{(n-1)} \cdot x_n - (x_1 + ... + x_{n-1})}{n} \bigg)^2}{n} \\
\end{align}
]

--

.font2[
With a small sample size, $(n-1)$ introduces .pink[**bias**]!
]

--

.font2[
Solution $\to$ .amber[Bessel's] correction
]

???

Bessel's correction: change the denominator $n \to (n-1)$

---

count: false

\begin{align}
\sigma^2 &= \frac{\displaystyle \sum_{i=1}^n(x_i - \mu)^2}{n} \tag{Parameter in the population}\\
\\
s^2     &= \frac{\displaystyle \sum_{i=1}^n(x_i - \color{#FFC107}{\bar{x}})^2}{\color{#FFC107}{n-1}} \tag{Statistics in sample}\\
\end{align}

???

- In a population with $n \to infty$, we do not need Bessel's correction
- In a sample with a finite element, we need to apply Bessel's correction

--

### How about .amber[standard deviation]?

--

### Simply take the .amber[square root] :)

---

template: overview
count: false

.bg-main1.column[.vmiddle.content[
- Type of data
- Descriptive statistics
- .amber[Data distribution]
- Visual examination
- Goodness of fit test
]]

---

layout: false
class: bg-main3

# Data distribution

.font2[
- Probability function
- Discrete: binomial, geometry, hypergeometry, Poisson, ...
- .amber[Continuous]: .amber[normal], gamma, $\color{#FFC107}{\chi^2}$, exponential, ...
]

???

- Probability function takes a parameter to define how a distribution behave
- We will focus on continuous distributions
- Or more specifically on normal and $\chi^2$ distribution (and their variants!)

---

class: bg-main3

# Normal distribution

.font2[
\begin{align}
P(X=x) &= \frac{1}{\color{#FFC107}{\sigma}\sqrt{2\pi}}e^{-\frac{1}{2} \bigg( \frac{x - \color{#FFC107}{\mu}}{\color{#FFC107}{\sigma}} \bigg)^2} \\
X & \sim N(\color{#FFC107}{\mu}, \color{#FFC107}{\sigma})
\end{align}
]

--

.font2[
- Parameters $\color{#FFC107}{\mu}$ and $\color{#FFC107}{\sigma}$ completely describes the distribution
- What makes it so special?
]

???

- Its statistics (or parameter in population) describes the probability density
- Meaning that, both the central tendency and spread fully explain the
  distribution
- Normal distribution is ubiquitous
- $X \in \mathbb{R}: - \infty < x < \infty$
- 68-95-99 rule in all normally-distributed data

---

count: false
class: bg-main3

```{r dist.norm.mu}

x <- seq(-20, 120, 0.05)

tbl <- data.frame(
	x = rep(x, times=5),
	y = c(dnorm(x, 50, 15), dnorm(x, 50, 20), dnorm(x, 50, 25), dnorm(x, 65, 20), dnorm(x, 35, 20)),
	mu = rep(c(50, 50, 50, 60, 40), each=length(x)) %>% as.character(),
	sigma = rep(c(10, 20, 30, 20, 20), each=length(x)) %>% as.character()
)

ggplot(tbl, aes(x=x, y=y)) + theme_minimal() +
	geom_line(aes(linetype=mu, color=sigma), size=rep(c(0.8, 2, 2, 2, 0.8), each=length(x))) +
	labs(x="Observable data", y="Probability") +
	scale_linetype_manual(values=c(2, 1, 2))

```

---

count: false
class: bg-main3

```{r dist.norm.sigma}

ggplot(tbl, aes(x=x, y=y)) + theme_minimal() +
	geom_line(aes(linetype=sigma, color=mu), size=rep(c(0.8, 2, 2, 2, 0.8), each=length(x))) +
	labs(x="Observable data", y="Probability") +
	scale_linetype_manual(values=c(2, 1, 2))

```

---

class: bg-main3

# Standardized normal distribution

.font2[
- A variant of normal distribution
- Has a specified value of $\mu=0$ and $\sigma=1$
- $Z \sim N(0, 1)$
- Standardization $\to$ scaling & centering
]

???

- Scaling: Division by the standard deviation
- Centering: A subtraction to the mean

---

count: false
class: bg-main3

```{r dist.std.norm}

x <- seq(-3, 3, 0.01)
tbl <- data.frame(x=x, y=dnorm(x, 0, 1))
x.off <- 0.1

ggplot(tbl, aes(x=x, y=y)) + theme_minimal() + geom_line(size=2) +
	geom_vline(xintercept={-3:3}[-4], linetype=3, size=1, color="grey50") +
	geom_vline(xintercept=0, linetype=3, size=1.5, color="#E91E63") +
	annotate("text", label="mu", x=0 + x.off, y=0.42, parse=TRUE, size=6) +
	annotate("text", label=c("SD -1", "SD 1"), x=c(-1, 1) + 2*x.off*c(1, -1), y=0.42) +
	annotate("text", label=c("SD -2", "SD 2"), x=c(-2, 2) + 2*x.off*c(1, -1), y=0.42) +
	annotate("text", label=c("SD -3", "SD 3"), x=c(-3, 3) + 2*x.off*c(1, -1), y=0.42) +
	labs(x="Observable data", y="Probability") +
	scale_x_continuous(breaks=-3:3)

```

---

class: bg-main3

# $\chi^2$ distribution

.font2[
\begin{align}
P(X=x) &= \frac{1}{\color{#E91E63}{\Gamma}(\frac{\color{#FFC107}{\nu}}{2})}x^{\frac{\color{#FFC107}{\nu}}{2}-1} e^{-\frac{x}{2}} \\
X & \sim \chi^2(\color{#FFC107}{\nu})
\end{align}

- Only depends on $\color{#FFC107}{\nu}$ degree of freedom
- A special case of Gamma distribution (notice the .pink[Gamma] function $\color{#E91E63}{\Gamma}$)
- What makes it so special?
]

???

- One of the mostly used distribution in statistics
- Example: Pearson's $\chi^2$, non-parametric test (Mann-Whitney U, Kruskal-Wallis H, etc.)
- $Z^2 \sim \chi^2(1)$
- Sum of square of $\nu$ independent variables $N_{1..\nu} \sim N(0,1)$
- Nice to know: Gamma function describes a factorial of positive complex real numbers

---

count: false
class: bg-main3

```{r dist.chisq}

x <- seq(0, 30, 0.05)

tbl <- data.frame(
	x = rep(x, times=4),
	y = c(dchisq(x, 1), dchisq(x, 2), dchisq(x, 3), dchisq(x, 5)),
	nu = rep(c(1, 2, 3, 5), each=length(x)) %>% as.character()
)

ggplot(tbl, aes(x=x, y=y)) + theme_minimal() +
	geom_line(aes(color=nu), size=1) + ylim(0, 1) +
	annotate("text", label="Vertical asymptote with 1 degree of freedom ", x=8.5, y=0.5, size=5) +
	geom_segment(x=1.3, y=0.5, xend=0.6, yend=0.5, size=0.1, arrow=arrow(length=unit(0.2, "cm"))) +
	labs(x="Observed value", y="Probability")

```

???

- More variable $\to$ higher degree of freedom $\to$ adjust the critical value
  to determine significance

---

class: bg-main3

# T distribution

.font2[
\begin{align}
P(X=x) &= \frac{\Gamma(\frac{\color{#FFC107}{\nu} + 1}{2})}{\sqrt{\color{#FFC107}{\nu} \pi} \cdot \Gamma(\frac{\color{#FFC107}{\nu}}{2})} \bigg(1 + \frac{x^2}{\color{#FFC107}{\nu}} \bigg)^{- \frac{\color{#FFC107}{\nu}+1}{2}} \\
X & \sim T(\color{#FFC107}{\nu})
\end{align}

- Approximating a standardized normal distribution
- Only depends on $\color{#FFC107}{\nu}$ degree of freedom
- What makes it so special?
]

???

- Estimate population mean $\mu$
- Small sample size, unknown population standard deviation $\sigma$
- The null distribution in T-Test (both one- and two-sample T-Test)
- With $\nu \to \infty$, $T \sim Z$

---

count: false
class: bg-main3

```{r dist.t}

x <- seq(-3, 3, 0.01)
tbl <- data.frame(
	x = rep(x, 4),
	y = c(dt(x, 1), dt(x, 5), dt(x, 10), dnorm(x, 0, 1)),
	nu = rep(c(1, 5, 10, "N(0,1)"), each=length(x))
)

ggplot(tbl, aes(x=x, y=y)) + theme_minimal() +
	geom_line(aes(color=nu)) +
	labs(x="Observed value", y="Probability")

```

???

Higher tails on lower degree of freedom allows better critical value adjustment
to avoid statistical error

---

class: bg-main3

# Summary

<img src="https://i.stack.imgur.com/HgpO4.jpg" width="98%">

Leemis LM, McQueston JT. Univariate distribution relationships. The American Statistician. 2008 Feb 1;62(1):45-53. doi:10.1198/000313008x270448

???

- What does it have anything to do with data visualization?
- By transforming our data, it may follow a different distribution
- Such a transformation is potentially essential to present a more coherent
  visualization

---

template: overview

.bg-main1.column[.vmiddle.content[
- Type of data
- Descriptive statistics
- Data distribution
- .amber[Visual examination]
- Goodness of fit test
]]

---

class: bg-main3

# Visually examining your data

.font2[
- Histogram
- Density plot
- QQ-plot
]

???

- Visual examination is the first step when conducting an exploratory analysis
- It aims to look for visual cues of how your data looks like
- It helps you to determine which test is more suitable to conduct

---

layout: true
class: bg-main3

# Data, please?

---

count: false

.font2[
For illustrative purposes, we will use the `iris` dataset
]

```{r str.iris}
str(iris)
```

--

.font2[
Limit our observation to `Sepal.Width` and `Petal.Width`
]

???

- Iris dataset has five columns describing iris flowers
- The first four are measurement of its petal and sepal
- The last column is the species of iris flower

---

count: false

.bg-white.content[
<br>
```{r dt.iris}
DT::datatable(iris)
```
]

---

layout: true
class: bg-main3

# Histogram

---

```{r hist.iris1}

ggplot(iris, aes(x=Sepal.Width, color=Species, fill=Species)) + theme_minimal() +
	geom_histogram(binwidth=0.2) +
	scale_x_continuous(breaks=seq(2, 4.5, 0.5)) +
	labs(title="Histogram with bin width = 0.2")

```

???

- `Setosa` occupies the right tail
- `Versicolor` occupies the left tail
- `Virginica` in the middle
- The problem with histogram: the number of bin determine how the data looks like

---

count: false

```{r hist.iris2}

ggplot(iris, aes(x=Sepal.Width, color=Species, fill=Species)) + theme_minimal() +
	geom_histogram(binwidth=0.5) +
	scale_x_continuous(breaks=seq(2, 4.5, 0.5)) +
	labs(title="Histogram with bin width = 0.5")

```

---

count: false

```{r hist.iris3}

ggplot(iris, aes(x=Sepal.Width, color=Species, fill=Species)) + theme_minimal() +
	geom_histogram(binwidth=0.1) +
	scale_x_continuous(breaks=seq(2, 4.5, 0.5)) +
	labs(title="Histogram with bin width = 0.1")

```

---

count: false

.font2[
- Histogram relies on how we set the bin width
- What can we do to circumvent the problem?
]

--

<img src="_fig/density-plot.jpeg" width="70%">

---

layout: true
class: bg-main3

# Density plot

---

```{r density.iris1}

ggplot(iris, aes(x=Sepal.Width, color=Species, fill=Species)) + theme_minimal() +
	geom_histogram(aes(y=..density..), binwidth=0.1, alpha=0.2) +
	geom_density(alpha=0.6, size=1.2) +
	scale_x_continuous(breaks=seq(2, 4.5, 0.5)) +
	labs(title="Histogram with a density plot")

```

???

There, there. We are almost done :) Now we need to visualize `Petal.Width` as well.

---

count: false

```{r density.iris2}

ggplot(iris, aes(x=Petal.Width, color=Species, fill=Species)) + theme_minimal() +
	geom_histogram(aes(y=..density..), binwidth=0.1, alpha=0.2) +
	geom_density(alpha=0.6, size=1.2) +
	scale_x_continuous(breaks=seq(2, 4.5, 0.5)) +
	labs(title="Histogram with a density plot")

```

---

count: false

.font2[
- It seems `Sepal.Width` has an approximately normal distribution
- While `Petal.Width` looks slightly off
- .amber[Conclusion:] We need to further investigate this issue
]

---

layout: true
class: bg-main3

# QQ-Plot

---

.font2[
- Sort your data based on its quantile
- Fit to a theoretical quantile
- You can use .amber[any] distribution to fit in your data
]

???

Meaning: you can use QQ-Plot to assess your data against any distribution

---

count: false

```{r qq.iris1}

par(mfrow=c(1, 3))
tmp <- tapply(iris$Sepal.Width, iris$Species, car::qqPlot)
title("Sepal width on three iris species")

```

---

count: false

```{r qq.iris2}

par(mfrow=c(1, 3))
tmp <- tapply(iris$Petal.Width, iris$Species, car::qqPlot)
title("Petal width on three iris species")

```

---

template: overview
count: false

.bg-main1.column[.vmiddle.content[
- Type of data
- Descriptive statistics
- Data distribution
- Visual examination
- .amber[Goodness of fit test]
]]

---

layout: false
class: bg-main3

# Goodness of fit

.font2[
- Measure how good our data fit a certain distribution
- .amber[Recall:] there are a plethora of distribution to test on
- Normality test is only a subset of it!
]

---

layout: true
class: bg-main3

# Kolmogorov-Smirnov Test

.font2[
- Non-parametric and distribution agnostic
- The most commonly used .amber[goodness of fit] test
- Has a favourable statistical power
- Often applicable to a small dataset
]

---

???

- Also known as KS-Test
- It measures the maximum distances between quantiles of two distributions
- The first distribution is *always* coming from our data (observable)
- The second distribution may come from our data
- ...or we can use a theoretical distribution (theoretical quantiles)
- The distance $D$ follows a Kolmogorov distribution
- We calculate the critical value using such a distribution

---

count: false

.bg-white[
<br>

```{r ks.test}

tapply(iris$Sepal.Width, iris$Species, ks.test, pnorm) %>%
	lapply(broom::tidy) %>% lapply(data.frame) %>% {do.call(rbind, .)} %>%
	knitr::kable(caption="Sepal width") %>% kable_paper()

```

<br>
]

---

count: false

.bg-white[
<br>

```{r ks.test2}

tapply(iris$Petal.Width, iris$Species, ks.test, pnorm) %>%
	lapply(broom::tidy) %>% lapply(data.frame) %>% {do.call(rbind, .)} %>%
	knitr::kable(caption="Petal width") %>% kable_paper()

```

<br>
]

---

layout: true
class: bg-main3

# Shapiro-Wilk Test

.font2[
- Non-parametric
- The most commonly used .amber[normality] test
- Good statistical power, quite robust in handling outliers
- Often applicable to a large dataset
]

---

???

- Practical question: how large is large enough for Shapiro-Wilk?
- No consensus on such a matter, usually KS-Test is good enough to handle up to
  100 sample size
- But we often employ KS-Test when we have $n \leqslant 30$
- With a higher number of data point $\to$ more chance to produce significant
  results in KS-Test compared to Shapiro-Wilk $\to$ type-I statistical error?
- Hint: in `R`, SW-Test can measure normality in a sample up to 5000 elements

---

count: false

.bg-white[
<br>

```{r shapiro.test}

tapply(iris$Sepal.Width, iris$Species, shapiro.test) %>%
	lapply(broom::tidy) %>% lapply(data.frame) %>% {do.call(rbind, .)} %>%
	knitr::kable(caption="Sepal width") %>% kable_paper()

```

<br>
]

---

count: false

.bg-white[
<br>

```{r shapiro.test2}

tapply(iris$Petal.Width, iris$Species, shapiro.test) %>%
	lapply(broom::tidy) %>% lapply(data.frame) %>% {do.call(rbind, .)} %>%
	knitr::kable(caption="Petal width") %>% kable_paper()

```

<br>
]

---

layout: true
class: bg-main3

# Anderson-Darling Test

.font2[
- Non-parametric
- Often used in econometrics
- Able to handle .amber[higher counts on tails]
]

---

???

- More lenient to accept probability deviation compared to the Shapiro-Wilk
  test
---

count: false

.bg-white[
<br>

```{r ad.test}

tapply(iris$Sepal.Width, iris$Species, nortest::ad.test) %>%
	lapply(broom::tidy) %>% lapply(data.frame) %>% {do.call(rbind, .)} %>%
	knitr::kable(caption="Sepal width") %>% kable_paper()

```

<br>
]

---

count: false

.bg-white[
<br>

```{r ad.test2}

tapply(iris$Petal.Width, iris$Species, nortest::ad.test) %>%
	lapply(broom::tidy) %>% lapply(data.frame) %>% {do.call(rbind, .)} %>%
	knitr::kable(caption="Petal width") %>% kable_paper()

```

<br>
]

